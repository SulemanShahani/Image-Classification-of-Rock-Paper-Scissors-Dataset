# Image Classification of Rock Paper Scissors Dataset

This project aims to build a machine learning model to classify images of rock, paper, and scissors hand gestures using the Rock Paper Scissors Dataset.

## Overview

The Rock Paper Scissors Dataset consists of images of hands in various positions representing rock, paper, and scissors gestures. The goal of this project is to develop a model that can accurately classify these hand gestures.

## Project Structure

The project structure is organized as follows:

- **data/**: Contains the dataset split into training, validation, and test sets.
- **notebooks/**: Contains the Jupyter notebook `practiceofimageclassification.ipynb` for the image classification project.
- **src/**: Contains Python source code for data loading (`data_loader.py`), model definition (`model.py`), and training (`train.py`).
- **README.md**: Provides an overview of the project and project structure (you're reading it right now!).

## Getting Started

To get started with this project, follow these steps:

1. Clone this repository to your local machine:

    ```bash
    git clone https://github.com/SulemanShahani/Image-Classification-of-Rock-Paper-Scissors-Dataset.git
    ```

2. Install the required dependencies:

    ```bash
    pip install -r requirements.txt
    ```

3. Download the Rock Paper Scissors Dataset and place it in the `data/` directory.

4. Open and run the Jupyter notebook `practiceofimageclassification.ipynb` to explore the project code and execute the image classification pipeline.

## Contributing

Contributions to this project are welcome! If you have any ideas for improvement or would like to contribute code, feel free to fork the repository and submit a pull request.
